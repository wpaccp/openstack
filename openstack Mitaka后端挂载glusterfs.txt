                     openstack Mitaka后端挂载glusterfs分布式存储设备配置
1.glusterfs分布式存储配置
省略
2.openstack Mitaka的安装和配置
2.1 预先配置
2.1.1 配置主机名和时间同步
# hostnamectl set-hostname linux-node1
# hostnamectl set-hostname linux-node2
2.1.2 修改/etc/hosts文件
# vim /etc/hosts
192.168.100.151 linux-node1
192.168.100.152 linux-node2
2.1.3 三个节点分别安装配置NTP
# yum install chrony -y 
# sed -i 's/^restrict\default\nomodify\notrap\nopeer\noquery/restrict\default\ nomodify\/' /etc/chrony.conf
# sed -i "/^# Please\consider\joining\the\pool/iserver\${HOSTNAME}\iburst" /etc/chrony.con
# systemctl enable chronyd.service
# systemctl start chronyd.service
2.2 安装openstack Mitaka
2.2.1 安装mariadb及rabbitmq
[root@linux-node1 ~]# yum install python-openstackclient -y
[root@linux-node1 ~]# yum install python-openstackclient -y
[root@linux-node1 ~]# yum install mariadb mariadb-server python2-PyMySQL -y
[root@linux-node1 ~]# vim /etc/my.cnf.d/mariadb_openstack.cnf
[mysqld]
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8
bind-address = 192.168.100.151

[root@linux-node1 ~]# systemctl enable mariadb.service
[root@linux-node1 ~]# systemctl restart mariadb.service
[root@linux-node1 ~]# systemctl status mariadb.service
[root@linux-node1 ~]# systemctl list-unit-files|grep mariadb.service
mariadb.service                             enabled
# mysql_secure_installation

[root@linux-node1 ~]# yum install rabbitmq-server -y
[root@linux-node1 ~]# systemctl enable rabbitmq-server.service 
[root@linux-node1 ~]# systemctl restart rabbitmq-server.service
[root@linux-node1 ~]# systemctl status rabbitmq-server.service
[root@linux-node1 ~]# systemctl list-unit-files |grep rabbitmq-server.service
rabbitmq-server.service                     enabled 

[root@linux-node1 ~]# rabbitmqctl add_user openstack openstack
Creating user "openstack" ...
[root@linux-node1 ~]# rabbitmqctl set_permissions openstack ".*" ".*" ".*"
Setting permissions for user "openstack" in vhost "/" ...
[root@linux-node1 ~]# netstat -ntlp |grep 5672
tcp        0      0 0.0.0.0:25672           0.0.0.0:*               LISTEN      5375/beam           
tcp6       0      0 :::5672                 :::*                    LISTEN      5375/beam
[root@linux-node1 ~]# /usr/lib/rabbitmq/bin/rabbitmq-plugins list
 Configured: E = explicitly enabled; e = implicitly enabled
 | Status:   [failed to contact rabbit@linux-node1 - status not shown]
 |/
[  ] amqp_client                       3.6.2
[  ] cowboy                            1.0.3
[  ] cowlib                            1.0.1
[  ] mochiweb                          2.13.1
[  ] rabbitmq_amqp1_0                  3.6.2
[  ] rabbitmq_auth_backend_ldap        3.6.2
[  ] rabbitmq_auth_mechanism_ssl       3.6.2
[  ] rabbitmq_consistent_hash_exchange 3.6.2
[  ] rabbitmq_event_exchange           3.6.2
[  ] rabbitmq_federation               3.6.2
[  ] rabbitmq_federation_management    3.6.2
[  ] rabbitmq_management               3.6.2
[  ] rabbitmq_management_agent         3.6.2
[  ] rabbitmq_management_visualiser    3.6.2
[  ] rabbitmq_mqtt                     3.6.2
[  ] rabbitmq_recent_history_exchange  1.2.1
[  ] rabbitmq_sharding                 0.1.0
[  ] rabbitmq_shovel                   3.6.2
[  ] rabbitmq_shovel_management        3.6.2
[  ] rabbitmq_stomp                    3.6.2
[  ] rabbitmq_tracing                  3.6.2
[  ] rabbitmq_web_dispatch             3.6.2
[  ] rabbitmq_web_stomp                3.6.2
[  ] rabbitmq_web_stomp_examples       3.6.2
[  ] sockjs                            0.3.4
[  ] webmachine                        1.10.3

[root@linux-node1 ~]# /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management
The following plugins have been enabled:
  mochiweb
  webmachine
  rabbitmq_web_dispatch
  amqp_client
  rabbitmq_management_agent
  rabbitmq_management

Applying plugin configuration to rabbit@linux-node1... failed.
 * Could not contact node rabbit@linux-node1.
   Changes will take effect at broker restart.
 * Options: --online  - fail if broker cannot be contacted.
            --offline - do not try to contact broker.

[root@linux-node1 ~]# systemctl restart rabbitmq-server.service
[root@linux-node1 ~]# systemctl status rabbitmq-server.service

[root@linux-node1 ~]# yum install memcached python-memcached -y
[root@linux-node1 ~]# vi /etc/sysconfig/memcached
PORT="11211"
USER="memcached"
MAXCONN="1024"
CACHESIZE="64"
OPTIONS="-l 192.168.100.151,::1"
[root@linux-node1 ~]# systemctl enable memcached.service && systemctl restart memcached.service
Created symlink from /etc/systemd/system/multi-user.target.wants/memcached.service to /usr/lib/systemd/system/memcached.service.
[root@linux-node1 ~]# systemctl status memcached.service

2.2.2 创建keystone数据库
[root@linux-node1 ~]# mysql -uroot -p123456 -e "CREATE DATABASE keystone;"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';" 
[root@linux-node1 ~]# mysql -uroot -p123456 -e "show databases;" 
[root@linux-node1 ~]# mysql -ukeystone -pkeystone -e 'show databases;'


[root@linux-node1 ~]# yum install openstack-keystone httpd mod_wsgi memcached python-memcached -y
[root@linux-node1 ~]# systemctl enable memcached.service
[root@linux-node1 ~]# systemctl restart memcached.service
[root@linux-node1 ~]# systemctl list-unit-files |grep memcached.service
memcached.service                           enabled 

[root@linux-node1 ~]#  yum install -y openstack-utils
[root@linux-node1 ~]# ADMIN_TOKEN=294a4c8a8a475f9b9836
[root@linux-node1 ~]# cp /etc/keystone/keystone.conf /etc/keystone/keystone.conf.bak
[root@linux-node1 ~]# >/etc/keystone/keystone.conf

# openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token $ADMIN_TOKEN 
# openstack-config --set /etc/keystone/keystone.conf database connection mysql://keystone:keystone@linux-node1/keystone
# openstack-config --set /etc/keystone/keystone.conf token provider fernet 

[root@linux-node1 ~]# su -s /bin/sh -c "keystone-manage db_sync" keystone
[root@linux-node1 ~]# mysql -ukeystone -pkeystone -e 'use keystone;show tables;'
[root@linux-node1 ~]# keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
[root@linux-node1 ~]# cp /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.bak
[root@linux-node1 ~]# sed -i "s/#ServerName www.example.com:80/ServerName linux-node1/" /etc/httpd/conf/httpd.conf
[root@linux-node1 ~]# grep "ServerName linux-node1" /etc/httpd/conf/httpd.conf

[root@linux-node1 ~]# vim /etc/httpd/conf.d/wsgi-keystone.conf
Listen 5000
Listen 35357

<VirtualHost *:5000>
WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
WSGIProcessGroup keystone-public
WSGIScriptAlias / /usr/bin/keystone-wsgi-public
WSGIApplicationGroup %{GLOBAL}
WSGIPassAuthorization On
<IfVersion >= 2.4>
ErrorLogFormat "%{cu}t %M"
</IfVersion>
ErrorLog /var/log/httpd/keystone-error.log
CustomLog /var/log/httpd/keystone-access.log combined
<Directory /usr/bin>
<IfVersion >= 2.4>
Require all granted
</IfVersion>
<IfVersion < 2.4>
Order allow,deny
Allow from all
</IfVersion>
</Directory>
</VirtualHost>

<VirtualHost *:35357>
WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
WSGIProcessGroup keystone-admin
WSGIScriptAlias / /usr/bin/keystone-wsgi-admin
WSGIApplicationGroup %{GLOBAL}
WSGIPassAuthorization On
<IfVersion >= 2.4>
ErrorLogFormat "%{cu}t %M"
</IfVersion>
ErrorLog /var/log/httpd/keystone-error.log
CustomLog /var/log/httpd/keystone-access.log combined
<Directory /usr/bin>
<IfVersion >= 2.4>
Require all granted
</IfVersion>
<IfVersion < 2.4>
Order allow,deny
Allow from all
</IfVersion>
</Directory>
</VirtualHost>
[root@linux-node1 ~]# systemctl enable httpd.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. 
[root@linux-node1 ~]# systemctl start httpd.service
[root@linux-node1 ~]# systemctl status httpd.service
[root@linux-node1 ~]# systemctl list-unit-files |grep httpd.service
httpd.service                               enabled 

[root@linux-node1 ~]# export OS_TOKEN=294a4c8a8a475f9b9836
[root@linux-node1 ~]# export OS_URL=http://linux-node1:35357/v3
[root@linux-node1 ~]# export OS_IDENTITY_API_VERSION=3

[root@linux-node1 ~]# openstack service create --name keystone --description "OpenStack Identity" identity
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Identity               |
| enabled     | True                             |
| id          | 1fcf14f10937425bacff54569e02e651 |
| name        | keystone                         |
| type        | identity                         |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne identity public http://linux-node1:5000/v3
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 5d94fdb826f44c60ab0dce7d6cb7fe80 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 1fcf14f10937425bacff54569e02e651 |
| service_name | keystone                         |
| service_type | identity                         |
| url          | http://linux-node1:5000/v3       |
+--------------+----------------------------------+
# [root@linux-node1 ~]# openstack endpoint create --region RegionOne identity internal http://linux-node1:5000/v3
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 0d048b4e87d64be0903980c9e8ee3c7e |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 1fcf14f10937425bacff54569e02e651 |
| service_name | keystone                         |
| service_type | identity                         |
| url          | http://-node1:5000/v3            |
+--------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne identity admin http://linux-node1:35357/v3
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 378cd2f6bd20413aa5c953b9211924e9 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 1fcf14f10937425bacff54569e02e651 |
| service_name | keystone                         |
| service_type | identity                         |
| url          | http://linux-node1:35357/v3      |
+--------------+----------------------------------+

#[root@linux-node1 ~]#  openstack domain create --description "Default Domain" default
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Default Domain                   |
| enabled     | True                             |
| id          | 723db0cf79e345eb842cfca02ee16403 |
| name        | default                          |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack project create --domain default --description "Admin Project" admin
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Admin Project                    |
| domain_id   | 723db0cf79e345eb842cfca02ee16403 |
| enabled     | True                             |
| id          | 27c82f07c93248a18eb09c0151bed843 |
| is_domain   | False                            |
| name        | admin                            |
| parent_id   | 723db0cf79e345eb842cfca02ee16403 |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack user create --domain default admin --password admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | 723db0cf79e345eb842cfca02ee16403 |
| enabled   | True                             |
| id        | a4441bee92994352a80d08b02ade6b4c |
| name      | admin                            |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role create admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | 13ba8ef2b31348b1b6f0124a8aa092bd |
| name      | admin                            |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role add --project admin --user admin admin

[root@linux-node1 ~]# openstack project create --domain default --description "Service Project" service
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Service Project                  |
| domain_id   | 723db0cf79e345eb842cfca02ee16403 |
| enabled     | True                             |
| id          | b413a152fa1244ed813f83156b2c8bbf |
| is_domain   | False                            |
| name        | service                          |
| parent_id   | 723db0cf79e345eb842cfca02ee16403 |
+-------------+----------------------------------+

[root@linux-node1 ~]# openstack project create --domain default --description "Demo Project" demo
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Demo Project                     |
| domain_id   | 723db0cf79e345eb842cfca02ee16403 |
| enabled     | True                             |
| id          | eab84aac714241658b6ecf2f4932d51f |
| is_domain   | False                            |
| name        | demo                             |
| parent_id   | 723db0cf79e345eb842cfca02ee16403 |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack user create --domain default demo --password demo
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | 723db0cf79e345eb842cfca02ee16403 |
| enabled   | True                             |
| id        | 19131c1e213b4a4898874255a9aa3ec8 |
| name      | demo                             |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role create user
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | bc2c4219e3eb4e7ea3ace14ea9658293 |
| name      | user                             |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role add --project demo --user demo user

[root@linux-node1 ~]# unset OS_TOKEN OS_URL
[root@linux-node1 ~]# openstack --os-auth-url http://linux-node1:35357/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name admin --os-username admin token issue --os-password admin
+------------+--------------------------------------------------------------------------------------------------------------+
| Field      | Value                                                                                                        |
+------------+--------------------------------------------------------------------------------------------------------------+
| expires    | 2018-09-04T00:49:20.918340Z                                                                                  |
| id         | gAAAAABbjciBhe7vE6K06xkWvQ_rF1-dxnwm0tmoiQ0veSbCTtEtW_60IUB9IwdQzQLpvt5fcaoozYuvRVesp4iYN977U-WF_-           |
|            | _uajM1VQsIxFB3DUfDDwhEhFVHClZCHKhbKJxmMxJU3D-jGvj2tgHk9Pb_EbhpV6pDe-DNjWod35kk2YTHY-c                        |
| project_id | 27c82f07c93248a18eb09c0151bed843                                                                             |
| user_id    | a4441bee92994352a80d08b02ade6b4c                                                                             |
+------------+--------------------------------------------------------------------------------------------------------------+
[root@linux-node1 ~]# openstack --os-auth-url http://linux-node1:5000/v3 --os-project-domain-name default --os-user-domain-name default --os-project-name demo --os-username demo token issue --os-password demo
+------------+--------------------------------------------------------------------------------------------------------------+
| Field      | Value                                                                                                        |
+------------+--------------------------------------------------------------------------------------------------------------+
| expires    | 2018-09-04T00:50:19.900743Z                                                                                  |
| id         | gAAAAABbjci8twa3echwmqEKTeQ79RoaQ-r8FFIQNsNOfz-A4QsZZ9thUzdKkAKuaQUupchbgjE0OhdLP2U3ofL_N5Ok129bqIeUDW045Imk |
|            | DZPAUUM_U0s4iKqtYHOLmzwo2dKHq3z6n5IkFoh81A5t6NGkdNnZiGKrxlmbNZyqsTyrtusqBlI                                  |
| project_id | eab84aac714241658b6ecf2f4932d51f                                                                             |
| user_id    | 19131c1e213b4a4898874255a9aa3ec8                                                                             |
+------------+--------------------------------------------------------------------------------------------------------------+

[root@linux-node1 ~]# vim /root/admin-openrc.sh
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_AUTH_URL=http://linux-node1:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_PASSWORD=admin

[root@linux-node1 ~]#  vim /root/demo-openrc.sh
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=demo
export OS_USERNAME=demo
export OS_AUTH_URL=http://linux-node1:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_PASSWORD=demo

2.2.4 安装glance
[root@linux-node1 ~]# mysql -uroot -p123456 -e "CREATE DATABASE glance;"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT  ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance';"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';"
[root@linux-node1 ~]# mysql -uglance -pglance -e 'show databases;'

[root@linux-node1 ~]# source admin-openrc.sh 
[root@linux-node1 ~]# openstack user create --domain default glance --password glance
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | 723db0cf79e345eb842cfca02ee16403 |
| enabled   | True                             |
| id        | 66abd490f8c5472a9a72b248cc6ebb19 |
| name      | glance                           |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role add --project service --user glance admin

[root@linux-node1 ~]# openstack service create --name glance --description "OpenStack Image service" image
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Image service          |
| enabled     | True                             |
| id          | 02ce970220914a2686f3ff97772ebc29 |
| name        | glance                           |
| type        | image                            |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne image public http://linux-node1:9292 
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 99cba41536c14eaf9f452a20a30ce0a3 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 02ce970220914a2686f3ff97772ebc29 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://linux-node1:9292          |
+--------------+----------------------------------+ 
[root@linux-node1 ~]# openstack endpoint create --region RegionOne image internal http://linux-node1:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 41c7a638abe94287bcc9d5424437251b |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 02ce970220914a2686f3ff97772ebc29 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://linux-node1:9292          |
+--------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne image admin http://linux-node1:9292
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | f4cc15dc83aa464aabd7c0f4352ddf12 |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 02ce970220914a2686f3ff97772ebc29 |
| service_name | glance                           |
| service_type | image                            |
| url          | http://linux-node1:9292          |
+--------------+----------------------------------+

[root@linux-node1 ~]# yum install openstack-glance python-glance python-glanceclient -y

[root@linux-node1 ~]# cp  /etc/glance/glance-api.conf  /etc/glance/glance-api.conf.bak
[root@linux-node1 ~]# > /etc/glance/glance-api.conf
openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:glance@linux-node1/glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://linux-node1:35357
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service
openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone
openstack-config --set /etc/glance/glance-api.conf glance_store stores file,http
openstack-config --set /etc/glance/glance-api.conf glance_store default_store file
openstack-config --set /etc/glance/glance-api.conf glance_store filesystem_store_datadir /var/lib/glance/images/

[root@linux-node1 ~]# cp /etc/glance/glance-registry.conf /etc/glance/glance-registry.conf.bak
[root@linux-node1 ~]# >/etc/glance/glance-registry.conf
openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:glance@linux-node1/glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://linux-node1:35357
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password glance
openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone

[root@linux-node1 ~]#  su -s /bin/sh -c "glance-manage db_sync" glance
Option "verbose" from group "DEFAULT" is deprecated for removal.  Its value may be silently ignored in the future.
/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/enginefacade.py:1056: OsloDBDeprecationWarning: EngineFacade is deprecated; please use oslo_db.sqlalchemy.enginefacade
  expire_on_commit=expire_on_commit, _conf=conf)
/usr/lib/python2.7/site-packages/pymysql/cursors.py:146: Warning: Duplicate index 'ix_image_properties_image_id_name' defined on the table 'glance.image_properties'. This is deprecated and will be disallowed in a future release.
  result = self._query(query)
[root@linux-node1 ~]# mysql -uglance -pglance -e 'use glance;show tables;'

[root@linux-node1 ~]# systemctl enable openstack-glance-api.service openstack-glance-registry.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-glance-api.service to /usr/lib/systemd/system/openstack-glance-api.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-glance-registry.service to /usr/lib/systemd/system/openstack-glance-registry.service. 
[root@linux-node1 ~]# systemctl restart openstack-glance-api.service openstack-glance-registry.service
[root@linux-node1 ~]# systemctl status openstack-glance-api.service openstack-glance-registry.service

[root@linux-node1 ~]# echo " " >> /root/admin-openrc && \
> echo " " >> /root/demo-openrc && \
> echo "export OS_IMAGE_API_VERSION=2" | tee -a /root/admin-openrc /root/demo-openrc
export OS_IMAGE_API_VERSION=2

[root@linux-node1 ~]# wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
[root@linux-node1 ~]# source /root/admin-openrc
[root@linux-node1 ~]# glance image-create --name "cirros-0.3.4-x86_64" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --visibility public --progress
[=============================>] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     |
| container_format | bare                                 |
| created_at       | 2018-09-04T00:28:37Z                 |
| disk_format      | qcow2                                |
| id               | d896eb70-1ce5-4710-9d5b-eba06772a40b |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros-0.3.4-x86_64                  |
| owner            | 27c82f07c93248a18eb09c0151bed843     |
| protected        | False                                |
| size             | 13287936                             |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2018-09-04T00:28:38Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+
[root@linux-node1 ~]# glance image-list
+--------------------------------------+---------------------+
| ID                                   | Name                |
+--------------------------------------+---------------------+
| d896eb70-1ce5-4710-9d5b-eba06772a40b | cirros-0.3.4-x86_64 |
+--------------------------------------+---------------------+

2.2.5 安装Nova
[root@linux-node1 ~]# mysql -uroot -p123456 -e "CREATE DATABASE nova;"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "CREATE DATABASE nova_api;"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'nova';"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'nova';"

[root@linux-node1 ~]# source /root/admin-openrc.sh 
[root@linux-node1 ~]# openstack user create --domain default nova --password nova
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | 723db0cf79e345eb842cfca02ee16403 |
| enabled   | True                             |
| id        | 9c786d90854d4ce09f204c92fcf06027 |
| name      | nova                             |
+-----------+----------------------------------+

[root@linux-node1 ~]# openstack role add --project service --user nova admin

[root@linux-node1 ~]# openstack service create --name nova --description "OpenStack Compute" compute
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Compute                |
| enabled     | True                             |
| id          | 826e52f64a2e4015887905b20ba4f954 |
| name        | nova                             |
| type        | compute                          |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne compute public http://linux-node1:8774/v2.1/%\(tenant_id\)s
+--------------+--------------------------------------------+
| Field        | Value                                      |
+--------------+--------------------------------------------+
| enabled      | True                                       |
| id           | a0eb2b529bb648dc9d0356114036159a           |
| interface    | public                                     |
| region       | RegionOne                                  |
| region_id    | RegionOne                                  |
| service_id   | 826e52f64a2e4015887905b20ba4f954           |
| service_name | nova                                       |
| service_type | compute                                    |
| url          | http://linux-node1:8774/v2.1/%(tenant_id)s |
+--------------+--------------------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne compute internal http://linux-node1:8774/v2.1/%\(tenant_id\)s
+--------------+--------------------------------------------+
| Field        | Value                                      |
+--------------+--------------------------------------------+
| enabled      | True                                       |
| id           | 232f3f4cc53a4ffeb7efcc5ad0a3df2a           |
| interface    | internal                                   |
| region       | RegionOne                                  |
| region_id    | RegionOne                                  |
| service_id   | 826e52f64a2e4015887905b20ba4f954           |
| service_name | nova                                       |
| service_type | compute                                    |
| url          | http://linux-node1:8774/v2.1/%(tenant_id)s |
+--------------+--------------------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne compute admin http://linux-node1:8774/v2.1/%\(tenant_id\)s
 
+--------------+--------------------------------------------+
| Field        | Value                                      |
+--------------+--------------------------------------------+
| enabled      | True                                       |
| id           | 6ea2714f30924d1ebf3ba29deb7b7a25           |
| interface    | admin                                      |
| region       | RegionOne                                  |
| region_id    | RegionOne                                  |
| service_id   | 826e52f64a2e4015887905b20ba4f954           |
| service_name | nova                                       |
| service_type | compute                                    |
| url          | http://linux-node1:8774/v2.1/%(tenant_id)s |
+--------------+--------------------------------------------+
[root@linux-node1 ~]# yum install -y openstack-nova-api openstack-nova-cert openstack-nova-conductor openstack-nova-console openstacknova-novncproxy openstack-nova-scheduler

[root@linux-node1 ~]# cp /etc/nova/nova.conf /etc/nova/nova.conf.bak
[root@linux-node1 ~]# > /etc/nova/nova.conf
openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:nova@linux-node1/nova
openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:nova@linux-node1/nova_api
openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbit
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host ${HOSTNAME}
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password  openstack
openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://linux-node1:35357
openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service
openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova
openstack-config --set /etc/nova/nova.conf keystone_authtoken password nova
openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.100.151
openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True
openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 192.168.100.151
openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.100.151
openstack-config --set /etc/nova/nova.conf glance api_servers http://linux-node1:9292
openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0
openstack-config --set /etc/nova/nova.conf vnc enabled True
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://192.168.100.151:6080/vnc_auto.html

[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage api_db sync" nova
[root@linux-node1 ~]# su -s /bin/sh -c "nova-manage db sync" nova
[root@linux-node1 ~]# mysql -pnova -pnova -e 'use nova;show tables;'
[root@linux-node1 ~]# mysql -unova -pnova -e 'use nova_api;show tables;'
[root@linux-node1 ~]# systemctl enable openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-nova-novncproxy.service to /usr/lib/systemd/system/openstack-nova-novncproxy.service.
[root@linux-node1 ~]# systemctl restart openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
[root@linux-node1 ~]# systemctl status openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
[root@linux-node1 ~]# systemctl list-unit-files |grep openstack-nova-*
openstack-nova-api.service                  enabled 
openstack-nova-cert.service                 enabled 
openstack-nova-compute.service              disabled
openstack-nova-conductor.service            enabled 
openstack-nova-console.service              disabled
openstack-nova-consoleauth.service          enabled 
openstack-nova-metadata-api.service         disabled
openstack-nova-novncproxy.service           enabled 
openstack-nova-os-compute-api.service       disabled
openstack-nova-scheduler.service            enabled 
openstack-nova-xvpvncproxy.service          disabled

[root@linux-node1 ~]# yum install openstack-nova-compute -y
[root@linux-node1 ~]# openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu

[root@linux-node1 ~]# systemctl enable libvirtd.service openstack-nova-compute.service
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-nova-compute.service to /usr/lib/systemd/system/openstack-nova-compute.service.
[root@linux-node1 ~]# systemctl start libvirtd.service openstack-nova-compute.service
[root@linux-node1 ~]# systemctl status libvirtd.service openstack-nova-compute.service

[root@linux-node1 ~]# source /root/admin-openrc.sh
[root@linux-node1 ~]# nova service-list 
+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host        | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-consoleauth | linux-node1 | internal | enabled | up    | 2018-09-04T01:16:12.000000 | -               |
| 2  | nova-scheduler   | linux-node1 | internal | enabled | up    | 2018-09-04T01:16:12.000000 | -               |
| 3  | nova-conductor   | linux-node1 | internal | enabled | up    | 2018-09-04T01:16:12.000000 | -               |
| 4  | nova-cert        | linux-node1 | internal | enabled | up    | 2018-09-04T01:16:13.000000 | -               |
| 7  | nova-compute     | linux-node1 | nova     | enabled | up    | 2018-09-04T01:16:14.000000 | -               |
+----+------------------+-------------+----------+---------+-------+----------------------------+-----------------+
[root@linux-node1 ~]# nova endpoints
/usr/lib/python2.7/site-packages/novaclient/v2/shell.py:4205: UserWarning: nova endpoints is deprecated, use openstack catalog list instead
  "nova endpoints is deprecated, use openstack catalog list instead")
WARNING: glance has no endpoint in ! Available endpoints for this service:
+-----------+----------------------------------+
| glance    | Value                            |
+-----------+----------------------------------+
| id        | 41c7a638abe94287bcc9d5424437251b |
| interface | internal                         |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://linux-node1:9292          |
+-----------+----------------------------------+
+-----------+----------------------------------+
| glance    | Value                            |
+-----------+----------------------------------+
| id        | 99cba41536c14eaf9f452a20a30ce0a3 |
| interface | public                           |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://linux-node1:9292          |
+-----------+----------------------------------+
+-----------+----------------------------------+
| glance    | Value                            |
+-----------+----------------------------------+
| id        | f4cc15dc83aa464aabd7c0f4352ddf12 |
| interface | admin                            |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://linux-node1:9292          |
+-----------+----------------------------------+
WARNING: keystone has no endpoint in ! Available endpoints for this service:
+-----------+----------------------------------+
| keystone  | Value                            |
+-----------+----------------------------------+
| id        | 0d048b4e87d64be0903980c9e8ee3c7e |
| interface | internal                         |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://-node1:5000/v3            |
+-----------+----------------------------------+
+-----------+----------------------------------+
| keystone  | Value                            |
+-----------+----------------------------------+
| id        | 378cd2f6bd20413aa5c953b9211924e9 |
| interface | admin                            |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://linux-node1:35357/v3      |
+-----------+----------------------------------+
+-----------+----------------------------------+
| keystone  | Value                            |
+-----------+----------------------------------+
| id        | 5d94fdb826f44c60ab0dce7d6cb7fe80 |
| interface | public                           |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://linux-node1:5000/v3       |
+-----------+----------------------------------+
WARNING: nova has no endpoint in ! Available endpoints for this service:
+-----------+---------------------------------------------------------------+
| nova      | Value                                                         |
+-----------+---------------------------------------------------------------+
| id        | 232f3f4cc53a4ffeb7efcc5ad0a3df2a                              |
| interface | internal                                                      |
| region    | RegionOne                                                     |
| region_id | RegionOne                                                     |
| url       | http://linux-node1:8774/v2.1/27c82f07c93248a18eb09c0151bed843 |
+-----------+---------------------------------------------------------------+
+-----------+---------------------------------------------------------------+
| nova      | Value                                                         |
+-----------+---------------------------------------------------------------+
| id        | 6ea2714f30924d1ebf3ba29deb7b7a25                              |
| interface | admin                                                         |
| region    | RegionOne                                                     |
| region_id | RegionOne                                                     |
| url       | http://linux-node1:8774/v2.1/27c82f07c93248a18eb09c0151bed843 |
+-----------+---------------------------------------------------------------+
+-----------+---------------------------------------------------------------+
| nova      | Value                                                         |
+-----------+---------------------------------------------------------------+
| id        | a0eb2b529bb648dc9d0356114036159a                              |
| interface | public                                                        |
| region    | RegionOne                                                     |
| region_id | RegionOne                                                     |
| url       | http://linux-node1:8774/v2.1/27c82f07c93248a18eb09c0151bed843 |
+-----------+---------------------------------------------------------------+

2.2.6 安装Cinder
[root@linux-node1 ~]# mysql -uroot -p123456 -e "CREATE DATABASE cinder;"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';"
[root@linux-node1 ~]# mysql -ucinder -pcinder -e 'show databases;'

[root@linux-node1 ~]# source admin-openrc.sh 
[root@linux-node1 ~]# openstack user create --domain default cinder --password cinder
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | 6f2a4835f2c14860bc80c8386735e41f |
| enabled   | True                             |
| id        | c5e4a63d7781449e97999d2681d2f241 |
| name      | cinder                           |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role add --project service --user cinder admin

[root@linux-node1 ~]# openstack service create --name cinder --description "OpenStack Block Storage" volume
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage          |
| enabled     | True                             |
| id          | 1de4c40ac6ac4456b5188962dcce7422 |
| name        | cinder                           |
| type        | volume                           |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Block Storage          |
| enabled     | True                             |
| id          | 39ccaad472ff4bc1bdc1dc98967550c5 |
| name        | cinderv2                         |
| type        | volumev2                         |
+-------------+----------------------------------+

[root@linux-node1 ~]# openstack endpoint create --region RegionOne volume public http://linux-node1:8776/v1/%\(tenant_id\)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | bf4fd6316fbf43db8c94ada0aedc2ac4         |
| interface    | public                                   |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | 1de4c40ac6ac4456b5188962dcce7422         |
| service_name | cinder                                   |
| service_type | volume                                   |
| url          | http://linux-node1:8776/v1/%(tenant_id)s |
+--------------+------------------------------------------+
# [root@linux-node1 ~]# openstack endpoint create --region RegionOne volume internal http://linux-node1:8776/v1/%\(tenant_id\)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | ba94aff22fa24dbd8152a50adf09a2b3         |
| interface    | internal                                 |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | 1de4c40ac6ac4456b5188962dcce7422         |
| service_name | cinder                                   |
| service_type | volume                                   |
| url          | http://linux-node1:8776/v1/%(tenant_id)s |
+--------------+------------------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne volume admin http://linux-node1:8776/v1/%\(tenant_id\)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 7053fda362654a1da43cffd6ef866f18         |
| interface    | admin                                    |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | 1de4c40ac6ac4456b5188962dcce7422         |
| service_name | cinder                                   |
| service_type | volume                                   |
| url          | http://linux-node1:8776/v1/%(tenant_id)s |
+--------------+------------------------------------------+
# [root@linux-node1 ~]# openstack endpoint create --region RegionOne volumev2 public http://linux-node1:8776/v2/%\(tenant_id\)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | b58ba304ba7c4681bed8c80aae8b82f2         |
| interface    | public                                   |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | 39ccaad472ff4bc1bdc1dc98967550c5         |
| service_name | cinderv2                                 |
| service_type | volumev2                                 |
| url          | http://linux-node1:8776/v2/%(tenant_id)s |
+--------------+------------------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne volumev2 internal http://linux-node1:8776/v2/%\(tenant_id\)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 88d06acbe22a4e9eb945b3e05968f7b4         |
| interface    | internal                                 |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | 39ccaad472ff4bc1bdc1dc98967550c5         |
| service_name | cinderv2                                 |
| service_type | volumev2                                 |
| url          | http://linux-node1:8776/v2/%(tenant_id)s |
+--------------+------------------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne volumev2 admin http://linux-node1:8776/v2/%\(tenant_id\)s
+--------------+------------------------------------------+
| Field        | Value                                    |
+--------------+------------------------------------------+
| enabled      | True                                     |
| id           | 243bd9f0fed149d881a674c3df92226f         |
| interface    | admin                                    |
| region       | RegionOne                                |
| region_id    | RegionOne                                |
| service_id   | 39ccaad472ff4bc1bdc1dc98967550c5         |
| service_name | cinderv2                                 |
| service_type | volumev2                                 |
| url          | http://linux-node1:8776/v2/%(tenant_id)s |
+--------------+------------------------------------------+

[root@linux-node1 ~]# yum install openstack-cinder -y

openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/cinder/cinder.conf DEFAULT rpc_backend rabbit
openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 192.168.100.151
openstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:cinder@linux-node1/cinder
openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_host linux-node1
openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_userid openstack
openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_password openstack
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://linux-node1:35357
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type password
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name service
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinder
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken password cinder
openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp


[root@linux-node1 ~]# su -s /bin/sh -c "cinder-manage db sync" cinder
[root@linux-node1 ~]# mysql -ucinder -pcinder -e 'use cinder;show tables;'
[root@linux-node1 ~]# openstack-config --set /etc/nova/nova.conf cinder os_region_name RegionOne

[root@linux-node1 ~]# systemctl restart openstack-nova-api.service
[root@linux-node1 ~]# systemctl status openstack-nova-api.service

[root@linux-node1 ~]# systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-cinder-api.service to /usr/lib/systemd/system/openstack-cinder-api.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-cinder-scheduler.service to /usr/lib/systemd/system/openstack-cinder-scheduler.service.
[root@linux-node1 ~]# systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service
[root@linux-node1 ~]# systemctl status openstack-cinder-api.service openstack-cinder-scheduler.service

#安装和配置glusterfs
1 glusterfs的安装和配置
1.1 glusterfs的安装（二台服务器都要安装，并启动glusterfs服务）
[root@glusterfs1 ~]# yum search centos-release-gluster
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.aliyun.com
 * extras: mirrors.163.com
 * updates: mirrors.aliyun.com
============================================ N/S Matched: centos-release-gluster ============================================
centos-release-gluster312.noarch : Gluster 3.12 (Long Term Stable) packages from the CentOS Storage SIG repository
centos-release-gluster41.x86_64 : Gluster 4.1 (Long Term Stable) packages from the CentOS Storage SIG repository
[root@glusterfs1 ~]# yum install centos-release-gluster312.noarch
[root@glusterfs1 ~]# yum --enablerepo=centos-gluster*-test install glusterfs-server glusterfs-cli glusterfs-geo-replication
[root@glusterfs1 ~]# /etc/init.d/glusterd start
Starting glusterd:[  OK  ]
[root@glusterfs1 ~]# /etc/init.d/glusterd status
glusterd (pid  2086) is running...

[root@glusterfs2 ~]# mkfs.ext4 /dev/sdb
[root@glusterfs1 ~]# echo "/dev/sdb     /data ext4   defaults   0 0" >>/etc/fstab
[root@glusterfs1 ~]# mkdir /data
[root@glusterfs1 ~]# mount /dev/sdb /data
[root@glusterfs1 ~]# mount

[root@glusterfs2 ~]# yum search centos-release-gluster
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.aliyun.com
 * extras: mirrors.163.com
 * updates: mirrors.aliyun.com
============================================ N/S Matched: centos-release-gluster ============================================
centos-release-gluster312.noarch : Gluster 3.12 (Long Term Stable) packages from the CentOS Storage SIG repository
centos-release-gluster41.x86_64 : Gluster 4.1 (Long Term Stable) packages from the CentOS Storage SIG repository
[root@glusterfs2 ~]# yum install centos-release-gluster312.noarch
[root@glusterfs2 ~]# yum --enablerepo=centos-gluster*-test install glusterfs-server glusterfs-cli glusterfs-geo-replication
[root@glusterfs2 ~]# /etc/init.d/glusterd start
Starting glusterd:[  OK  ]
[root@glusterfs2 ~]# /etc/init.d/glusterd status
glusterd (pid  2086) is running...

[root@glusterfs2 ~]# mkfs.ext4 /dev/sdb
[root@glusterfs2 ~]# echo "/dev/sdb     /data ext4   defaults   0 0" >>/etc/fstab
[root@glusterfs2 ~]# mkdir /data
[root@glusterfs2 ~]# mount /dev/sdb /data
[root@glusterfs2 ~]# mount


1.2 glusterfs的配置（在其中一台服务器上执行如下命令)`
[root@glusterfs1 ~]# gluster peer probe glusterfs2
peer probe: success.  
[root@glusterfs1 ~]# gluster peer status
Number of Peers: 2

Hostname: glusterfs2
Uuid: 3750bcb0-5dc2-41c9-ab65-08c26c38be36
State: Peer in Cluster (Connected)

[root@glusterfs1 ~]# gluster volume create gv1 replica 2 linux-node1:/data linux-node2:/data force
volume create: gv1: success: please start the volume to access data
[root@glusterfs1 ~]# gluster peer status
Number of Peers: 1

Hostname: glusterfs2
Uuid: 5a67275e-0b9a-4842-9422-db88cbef22f5
State: Peer in Cluster (Connected)
[root@glusterfs1 ~]# gluster volume info 
Volume Name: gv1
Type: Replicate
Volume ID: d24f307e-7019-4e02-96d0-082939fdabaf
Status: Created
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: glusterfs1:/data
Brick2: glusterfs2:/data
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
[root@glusterfs1 ~]# gluster volume start gv1
volume start: gv1: success
[root@glusterfs1 ~]# gluster volume info
 
Volume Name: gv1
Type: Replicate
Volume ID: d24f307e-7019-4e02-96d0-082939fdabaf
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: glusterfs1:/data
Brick2: glusterfs2:/data
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off

测试效果
[root@glusterfs1 ~]# mount -t glusterfs linux-node1:/gv1 /mnt/
[root@glusterfs1 mnt]# touch 1.txt
[root@glusterfs1 mnt]# ls -l
total 5
-rw-r--r--. 1 root root    0 Aug 12 19:32 1.txt
-rw-r--r--. 1 root root    7 Aug 12 19:35 2.txt
drwx------. 2 root root 4096 Aug 12 19:13 lost+found

[root@glusterfs2 ~]# mount -t glusterfs linux-node2:/gv1 /mnt/
[root@glusterfs2 ~]# cd /mnt
[root@glusterfs2 mnt]# ls  -l
total 4
-rw-r--r--. 1 root root    0 Aug 12  2018 1.txt
drwx------. 2 root root 4096 Aug 12  2018 lost+found
[root@glusterfs2 mnt]# touch 2.txt
[root@glusterfs2 mnt]# echo wpaccp > 2.txt
[root@glusterfs2 mnt]# ls
1.txt  2.txt  lost+found

#设置相关权限

[root@linux-node1 ~]# chown -R cinder:cinder /data

[root@linux-node1 ~]# openstack-config --set /etc/cinder/cinder.conf DEFAULT volume_driver cinder.volume.drivers.glusterfs.GlusterfsDriver
[root@linux-node1 ~]# openstack-config --set /etc/cinder/cinder.conf DEFAULT glusterfs_shares_config /etc/cinder/shares.conf
[root@linux-node1 ~]# openstack-config --set /etc/cinder/cinder.conf DEFAULT glusterfs_mount_point_base /var/lib/cinder/volumes

[root@linux-node1 ~]# touch /etc/cinder/shares.conf
[root@linux-node1 ~]# vim /etc/cinder/shares.conf
linux-node1:/gv1

[root@linux-node1 ~]# chown -R cinder.cinder /etc/cinder/shares.conf
[root@linux-node1 ~]# chown -R cinder:cinder /var/lib/cinder/*
[root@linux-node1 ~]# systemctl enable openstack-cinder-volume.service
[root@linux-node1 ~]# systemctl start openstack-cinder-volume.service

[root@linux-node1 ~]# cinder service-list
+------------------+-------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |     Host    | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+-------------+------+---------+-------+----------------------------+-----------------+
| cinder-scheduler | linux-node1 | nova | enabled |   up  | 2018-09-06T15:18:52.000000 |        -        |
|  cinder-volume   | linux-node1 | nova | enabled |   up  | 2018-09-06T15:19:00.000000 |        -        |
+------------------+-------------+------+---------+-------+----------------------------+-----------------+
cinder list
[root@linux-node1 ~]# cinder type-create glusterfs
+--------------------------------------+-----------+-------------+-----------+
|                  ID                  |    Name   | Description | Is_Public |
+--------------------------------------+-----------+-------------+-----------+
| 7311badd-a76a-482d-9973-f97eb917c37e | glusterfs |      -      |    True   |
+--------------------------------------+-----------+-------------+-----------+
[root@linux-node1 ~]# cd /var/lib/cinder/volumes/
[root@linux-node1 volumes]# ll
total 4
drwxr-xr-x 2 cinder cinder    6 Sep  6 23:17 68cfe735747dd7d68166f4395d9f9947
drwxrwxr-x 5 cinder cinder 4096 Sep  6 23:29 7ec067eb9c9829e0defec361e5076a25
[root@linux-node1 volumes]# cd 7ec067eb9c9829e0defec361e5076a25/
[root@linux-node1 7ec067eb9c9829e0defec361e5076a25]# ll
total 533
-rw-r--r-- 1 cinder cinder          7 Sep  3 10:31 index.txt
drwx------ 2 cinder cinder       4096 Sep  3 10:19 lost+found
-rw-rw-rw- 1 qemu   qemu   2148073472 Sep  6 23:28 volume-dbed6484-6837-43f4-8b23-743409ffdf35

2.7 安装Neutron
[root@linux-node1 ~]# mysql -uroot -p123456 -e "CREATE DATABASE neutron;"
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';" 
[root@linux-node1 ~]# mysql -uroot -p123456 -e "GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron';"
[root@linux-node1 ~]# mysql -uneutron -pneutron -e 'show databases;'

[root@linux-node1 ~]# openstack user create --domain default neutron --password neutron
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | 723db0cf79e345eb842cfca02ee16403 |
| enabled   | True                             |
| id        | 12c49dc215cf45f88fd8afe67069f1b0 |
| name      | neutron                          |
+-----------+----------------------------------+
[root@linux-node1 ~]# openstack role add --project service --user neutron admin

[root@linux-node1 ~]# openstack service create --name neutron --description "OpenStack Networking" network
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | OpenStack Networking             |
| enabled     | True                             |
| id          | 14828b01e4ed4ecfa1a2ff4bd9ed53d7 |
| name        | neutron                          |
| type        | network                          |
+-------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne network public http://linux-node1:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | a0e8f26b1017465392b204a9b6306443 |
| interface    | public                           |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 14828b01e4ed4ecfa1a2ff4bd9ed53d7 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://linux-node1:9696          |
+--------------+----------------------------------+
[root@linux-node1 ~]# openstack endpoint create --region RegionOne network internal http://linux-node1:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 450d0e03d57246b0892eb4f553f3bccf |
| interface    | internal                         |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 14828b01e4ed4ecfa1a2ff4bd9ed53d7 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://linux-node1:9696          |
+--------------+----------------------------------+
[root@linux-node1 ~]#  openstack endpoint create --region RegionOne network admin http://linux-node1:9696
+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | d86d5f9776004a3782953e78252d909f |
| interface    | admin                            |
| region       | RegionOne                        |
| region_id    | RegionOne                        |
| service_id   | 14828b01e4ed4ecfa1a2ff4bd9ed53d7 |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://linux-node1:9696          |
+--------------+----------------------------------+

[root@linux-node1 ~]# yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y
[root@linux-node1 ~]# cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.bak
[root@linux-node1 ~]# > /etc/neutron/neutron.conf
openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:neutron@linux-node1/neutron
openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
openstack-config --set /etc/neutron/neutron.conf DEFAULT allow_overlapping_ips True
openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_host linux-node1
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstack
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstack 
openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://linux-node1:35357 
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutron
openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_status_changes True
openstack-config --set /etc/neutron/neutron.conf DEFAULT notify_nova_on_port_data_changes True
openstack-config --set /etc/neutron/neutron.conf nova auth_url http://linux-node1:35357
openstack-config --set /etc/neutron/neutron.conf nova auth_type password
openstack-config --set /etc/neutron/neutron.conf nova project_domain_name default
openstack-config --set /etc/neutron/neutron.conf nova user_domain_name default
openstack-config --set /etc/neutron/neutron.conf nova region_name RegionOne
openstack-config --set /etc/neutron/neutron.conf nova project_name service
openstack-config --set /etc/neutron/neutron.conf nova username nova
openstack-config --set /etc/neutron/neutron.conf nova password nova
openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp

[root@linux-node1 ~]# cp /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugins/ml2/ml2_conf.ini.bak
[root@linux-node1 ~]# >/etc/neutron/plugins/ml2/ml2_conf.ini
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan,vxlan 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers linuxbridge,l2population 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vxlan 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks provider
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vxlan vni_ranges 1:1000 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True

[root@linux-node1 ~]# cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak
[root@linux-node1 ~]# >/etc/neutron/plugins/ml2/linuxbridge_agent.ini
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan local_ip 192.168.100.151
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan l2_population True 
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini agent prevent_arp_spoofing True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group True 
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver \ neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

[root@linux-node1 ~]# cp /etc/neutron/l3_agent.ini /etc/neutron/l3_agent.ini.bak
[root@linux-node1 ~]# > /etc/neutron/l3_agent.ini
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriver 
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT external_network_bridge

[root@linux-node1 ~]# cp /etc/neutron/dhcp_agent.ini /etc/neutron/dhcp_agent.ini.bak
[root@linux-node1 ~]# > /etc/neutron/dhcp_agent.ini
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver neutron.agent.linux.interface.BridgeInterfaceDriver
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT verbose True

openstack-config --set /etc/nova/nova.conf neutron url http://linux-node1:9696 
openstack-config --set /etc/nova/nova.conf neutron auth_url http://linux-node1:35357 
openstack-config --set /etc/nova/nova.conf neutron auth_plugin password 
openstack-config --set /etc/nova/nova.conf neutron project_domain_id default 
openstack-config --set /etc/nova/nova.conf neutron user_domain_id default 
openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne
openstack-config --set /etc/nova/nova.conf neutron project_name service 
openstack-config --set /etc/nova/nova.conf neutron username neutron 
openstack-config --set /etc/nova/nova.conf neutron password neutron 
openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy True 
openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret 123456

[root@linux-node1 ~]# echo "dhcp-option-force=26,1450" >/etc/neutron/dnsmasq-neutron.conf

[root@linux-node1 ~]# cp /etc/neutron/metadata_agent.ini /etc/neutron/metadata_agent.ini.bak
[root@linux-node1 ~]# > /etc/neutron/metadata_agent.ini
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip linux-node1
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret 123456
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT verbose True

[root@linux-node1 ~]# ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini

[root@linux-node1 ~]# su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
[root@linux-node1 ~]# mysql -uneutron -pneutron -e 'use neutron;show tables;'

[root@linux-node1 ~]# systemctl restart openstack-nova-api.service
[root@linux-node1 ~]# systemctl status openstack-nova-api.service

[root@linux-node1 ~]# systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-server.service to /usr/lib/systemd/system/neutron-server.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-linuxbridge-agent.service to /usr/lib/systemd/system/neutron-linuxbridge-agent.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-dhcp-agent.service to /usr/lib/systemd/system/neutron-dhcp-agent.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-metadata-agent.service to /usr/lib/systemd/system/neutron-metadata-agent.service. 
[root@linux-node1 ~]# systemctl restart neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
[root@linux-node1 ~]# systemctl status neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service

[root@linux-node1 ~]# systemctl enable neutron-l3-agent.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-l3-agent.service to /usr/lib/systemd/system/neutron-l3-agent.service.
[root@linux-node1 ~]# systemctl start neutron-l3-agent.service
[root@linux-node1 ~]# systemctl status neutron-l3-agent.service

[root@linux-node1 ~]# source /root/admin-openrc.sh
[root@linux-node1 ~]# neutron ext-list
[root@linux-node1 ~]# neutron agent-list
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+
| id                 | agent_type         | host        | availability_zone | alive | admin_state_up | binary              |
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+
| 4d137128-f8b6-48ed | L3 agent           | linux-node1 | nova              | :-)   | True           | neutron-l3-agent    |
| -8172-82a471838f54 |                    |             |                   |       |                |                     |
| a7e1fe35-0aaf-     | Metadata agent     | linux-node1 |                   | :-)   | True           | neutron-metadata-   |
| 4cae-8e7e-         |                    |             |                   |       |                | agent               |
| f915042548fa       |                    |             |                   |       |                |                     |
| e0974bf1-315e-4eb4 | DHCP agent         | linux-node1 | nova              | :-)   | True           | neutron-dhcp-agent  |
| -a9a9-cda6158e6bac |                    |             |                   |       |                |                     |
| ef2f287e-7414-4626 | Linux bridge agent | linux-node1 |                   | :-)   | True           | neutron-            |
| -a783-5230a93c1d81 |                    |             |                   |       |                | linuxbridge-agent   |
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+


[root@linux-node1 ~]# source /root/demo-openrc.sh
[root@linux-node1 ~]# ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""
[root@linux-node1 ~]# openstack keypair create --public-key ~/.ssh/id_dsa.pub mykey
+-------------+-------------------------------------------------+
| Field       | Value                                           |
+-------------+-------------------------------------------------+
| fingerprint | 63:c3:bd:7b:6e:07:5f:48:74:75:f4:6c:89:f4:8d:cc |
| name        | mykey                                           |
| user_id     | 19131c1e213b4a4898874255a9aa3ec8                |
+-------------+-------------------------------------------------+

[root@linux-node1 ~]# openstack security group rule create --proto icmp default
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| id                    | 517ba93a-19ea-4076-a681-eb5722b55cca |
| ip_protocol           | icmp                                 |
| ip_range              | 0.0.0.0/0                            |
| parent_group_id       | 88694295-45c8-4eb9-9963-c6f975119f26 |
| port_range            |                                      |
| remote_security_group |                                      |
+-----------------------+--------------------------------------+
[root@linux-node1 ~]# openstack security group rule create --proto tcp --dst-port 22 default
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| id                    | 944750c1-5506-4819-98bd-36af8aa66614 |
| ip_protocol           | tcp                                  |
| ip_range              | 0.0.0.0/0                            |
| parent_group_id       | 88694295-45c8-4eb9-9963-c6f975119f26 |
| port_range            | 22:22                                |
| remote_security_group |                                      |
+-----------------------+--------------------------------------+

[root@linux-node1 ~]# systemctl restart libvirtd.service openstack-nova-compute.service 
[root@linux-node1 ~]# systemctl restart neutron-dhcp-agent neutron-l3-agent neutron-linuxbridge-agent neutron-metadata-agent neutron-server
[root@linux-node1 ~]# systemctl status neutron-dhcp-agent neutron-l3-agent neutron-linuxbridge-agent neutron-metadata-agent neutron-server

[root@linux-node1 ~]# source /root/admin-openrc.sh
[root@linux-node1 ~]# neutron --debug net-create --shared provider --router:external True --provider:network_type flat --provider:physical_network provider
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-09-04T02:10:10                  |
| description               |                                      |
| id                        | 93e02f49-83d0-46ba-8382-de21c74fd7db |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| is_default                | False                                |
| mtu                       | 1500                                 |
| name                      | provider                             |
| port_security_enabled     | True                                 |
| provider:network_type     | flat                                 |
| provider:physical_network | provider                             |
| provider:segmentation_id  |                                      |
| router:external           | True                                 |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | 27c82f07c93248a18eb09c0151bed843     |
| updated_at                | 2018-09-04T02:10:10                  |
+---------------------------+--------------------------------------+

[root@linux-node1 ~]# neutron subnet-create provider 192.168.100.0/24 --name provider-sub --allocation-pool start=192.168.100.220,end=192.168.100.230 --dns-nameserver 114.114.114.114 --gateway 192.168.100.1
Created a new subnet:
+-------------------+--------------------------------------------------------+
| Field             | Value                                                  |
+-------------------+--------------------------------------------------------+
| allocation_pools  | {"start": "192.168.100.220", "end": "192.168.100.230"} |
| cidr              | 192.168.100.0/24                                       |
| created_at        | 2018-09-04T02:12:45                                    |
| description       |                                                        |
| dns_nameservers   | 114.114.114.114                                        |
| enable_dhcp       | True                                                   |
| gateway_ip        | 192.168.100.1                                          |
| host_routes       |                                                        |
| id                | 35fbe85f-9ed6-460a-9f17-d5c09378e2a2                   |
| ip_version        | 4                                                      |
| ipv6_address_mode |                                                        |
| ipv6_ra_mode      |                                                        |
| name              | provider-sub                                           |
| network_id        | 93e02f49-83d0-46ba-8382-de21c74fd7db                   |
| subnetpool_id     |                                                        |
| tenant_id         | 27c82f07c93248a18eb09c0151bed843                       |
| updated_at        | 2018-09-04T02:12:45                                    |
+-------------------+--------------------------------------------------------+

[root@linux-node1 ~]# neutron net-create private --provider:network_type vxlan --router:external False --shared
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2018-09-04T02:13:27                  |
| description               |                                      |
| id                        | 3a574028-39e0-4762-aff9-777fa88f7a39 |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 1450                                 |
| name                      | private                              |
| port_security_enabled     | True                                 |
| provider:network_type     | vxlan                                |
| provider:physical_network |                                      |
| provider:segmentation_id  | 18                                   |
| router:external           | False                                |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | 27c82f07c93248a18eb09c0151bed843     |
| updated_at                | 2018-09-04T02:13:27                  |
+---------------------------+--------------------------------------+
[root@linux-node1 ~]# neutron subnet-create private --name internal-subnet --gateway 10.0.0.1 10.0.0.0/24
Created a new subnet:
+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {"start": "10.0.0.2", "end": "10.0.0.254"} |
| cidr              | 10.0.0.0/24                                |
| created_at        | 2018-09-04T02:13:57                        |
| description       |                                            |
| dns_nameservers   |                                            |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.0.1                                   |
| host_routes       |                                            |
| id                | af8c47fb-414d-40ea-bd6f-d81920eb451a       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              | internal-subnet                            |
| network_id        | 3a574028-39e0-4762-aff9-777fa88f7a39       |
| subnetpool_id     |                                            |
| tenant_id         | 27c82f07c93248a18eb09c0151bed843           |
| updated_at        | 2018-09-04T02:13:57                        |
+-------------------+--------------------------------------------+
[root@linux-node1 ~]# neutron agent-list
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+
| id                 | agent_type         | host        | availability_zone | alive | admin_state_up | binary              |
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+
| 4d137128-f8b6-48ed | L3 agent           | linux-node1 | nova              | :-)   | True           | neutron-l3-agent    |
| -8172-82a471838f54 |                    |             |                   |       |                |                     |
| a7e1fe35-0aaf-     | Metadata agent     | linux-node1 |                   | :-)   | True           | neutron-metadata-   |
| 4cae-8e7e-         |                    |             |                   |       |                | agent               |
| f915042548fa       |                    |             |                   |       |                |                     |
| e0974bf1-315e-4eb4 | DHCP agent         | linux-node1 | nova              | :-)   | True           | neutron-dhcp-agent  |
| -a9a9-cda6158e6bac |                    |             |                   |       |                |                     |
| ef2f287e-7414-4626 | Linux bridge agent | linux-node1 |                   | :-)   | True           | neutron-            |
| -a783-5230a93c1d81 |                    |             |                   |       |                | linuxbridge-agent   |
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+

2.8 安装Dashboard 
[root@linux-node1 ~]# yum install openstack-dashboard -y
[root@linux-node1 ~]# vi /etc/openstack-dashboard/local_settings
#配置dashboard运行在linux-node1上（controller为OS主机名） 
OPENSTACK_HOST = "linux-node1"
#配置允许登陆dashboard的主机 
ALLOWED_HOSTS = ['*', ]
#配置存储服务，添加下面内容
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
CACHES = {
'default': {
'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
'LOCATION': 'controller:11211',
},
}
#注意：注释掉其他的caches
#配置默认用dashboard创建的用户默认角色
OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

[root@linux-node1 ~]# systemctl enable httpd.service memcached.service
[root@linux-node1 ~]# systemctl restart httpd.service memcached.service
[root@linux-node1 ~]# systemctl status httpd.service memcached.service

3.Compute节点部署
# yum install -y openstack-selinux python-openstackclient yum-plugin-priorities openstack-nova-compute openstack-utils
3.1 配置nova
[root@linux-node2 ~]# cp /etc/nova/nova.conf /etc/nova/nova.conf.bak
[root@linux-node2 ~]# >/etc/nova/nova.conf
openstack-config --set /etc/nova/nova.conf DEFAULT rpc_backend rabbit
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_host linux-node1
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_userid openstack
openstack-config --set /etc/nova/nova.conf oslo_messaging_rabbit rabbit_password  openstack
openstack-config --set /etc/nova/nova.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://linux-node1:35357
openstack-config --set /etc/nova/nova.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service
openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova
openstack-config --set /etc/nova/nova.conf keystone_authtoken password nova
openstack-config --set /etc/nova/nova.conf DEFAULT my_ip 192.168.100.152
openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True
openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf vnc enabled True
openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0
openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address 192.168.100.152
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://192.168.100.151:6080/vnc_auto.html
openstack-config --set /etc/nova/nova.conf glance api_servers http://192.168.100.151:9292
openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu
openstack-config --set /etc/nova/nova.conf DEFAULT vif_plugging_is_fatal False
openstack-config --set /etc/nova/nova.conf DEFAULT vif_plugging_timeout 0

[root@linux-node2 ~]# systemctl enable libvirtd.service openstack-nova-compute.service
Created symlink from /etc/systemd/system/multi-user.target.wants/openstack-nova-compute.service to /usr/lib/systemd/system/openstack-nova-compute.service.
[root@linux-node2 ~]# systemctl start libvirtd.service openstack-nova-compute.service
[root@linux-node2 ~]# systemctl status libvirtd.service openstack-nova-compute.service

[root@linux-node2 ~]# cat <<END >/root/admin-openrc.sh 
> export OS_PROJECT_DOMAIN_NAME=default
> export OS_USER_DOMAIN_NAME=default
> export OS_PROJECT_NAME=admin
> export OS_USERNAME=admin
> export OS_PASSWORD=admin
> export OS_AUTH_URL=http://linux-node1:35357/v3
> export OS_IDENTITY_API_VERSION=3
> END

[root@linux-node2 ~]# cat <<END >/root/demo-openrc.sh 
> export OS_PROJECT_DOMAIN_NAME=default
> export OS_USER_DOMAIN_NAME=default
> export OS_PROJECT_NAME=demo
> export OS_USERNAME=demo
> export OS_PASSWORD=demo
> export OS_AUTH_URL=http://linux-node1:5000/v3
> export OS_IDENTITY_API_VERSION=3
> END

[root@linux-node2 ~]# source /root/admin-openrc.sh
[root@linux-node2 ~]# openstack compute service list
+----+------------------+-------------+----------+---------+-------+----------------------------+
| Id | Binary           | Host        | Zone     | Status  | State | Updated At                 |
+----+------------------+-------------+----------+---------+-------+----------------------------+
|  1 | nova-consoleauth | linux-node1 | internal | enabled | up    | 2018-09-04T03:45:29.000000 |
|  2 | nova-scheduler   | linux-node1 | internal | enabled | up    | 2018-09-04T03:45:29.000000 |
|  3 | nova-conductor   | linux-node1 | internal | enabled | up    | 2018-09-04T03:45:28.000000 |
|  4 | nova-cert        | linux-node1 | internal | enabled | up    | 2018-09-04T03:45:29.000000 |
|  7 | nova-compute     | linux-node1 | nova     | enabled | up    | 2018-09-04T03:45:30.000000 |
|  8 | nova-compute     | linux-node2 | nova     | enabled | up    | 2018-09-04T03:45:36.000000 |
+----+------------------+-------------+----------+---------+-------+----------------------------+

3.2 安装Neutron
[root@linux-node2 ~]# yum install openstack-neutron-linuxbridge ebtables ipset -y

[root@linux-node2 ~]# cp /etc/neutron/neutron.conf /etc/neutron/neutron.conf.bak
[root@linux-node2 ~]# > /etc/neutron/neutron.conf
# sed -i '/^connection/d' /etc/neutron/neutron.conf
openstack-config --set /etc/neutron/neutron.conf DEFAULT rpc_backend rabbit
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_host linux-node1
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_userid openstack
openstack-config --set /etc/neutron/neutron.conf oslo_messaging_rabbit rabbit_password openstack
openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://linux-node1:5000
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://linux-node1:35357
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers linux-node1:11211
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutron
openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp 

[root@linux-node2 ~]# cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak
[root@linux-node2 ~]# >/etc/neutron/plugins/ml2/linuxbridge_agent.ini
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini linux_bridge physical_interface_mappings provider:eth0
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan enable_vxlan True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan local_ip 192.168.100.152
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini vxlan l2_population True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup enable_security_group True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini securitygroup firewall_driver \ neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

openstack-config --set /etc/nova/nova.conf neutron url http://linux-node1:9696
openstack-config --set /etc/nova/nova.conf neutron auth_url http://linux-node1:35357
openstack-config --set /etc/nova/nova.conf neutron auth_type password
openstack-config --set /etc/nova/nova.conf neutron project_domain_name default
openstack-config --set /etc/nova/nova.conf neutron user_domain_name default
openstack-config --set /etc/nova/nova.conf neutron region_name RegionOne
openstack-config --set /etc/nova/nova.conf neutron project_name service
openstack-config --set /etc/nova/nova.conf neutron username neutron
openstack-config --set /etc/nova/nova.conf neutron password neutron

[root@linux-node2 ~]# systemctl restart openstack-nova-compute.service
# systemctl enable neutron-linuxbridge-agent.service && systemctl restart neutron-linuxbridge-agent.service
[root@linux-node2 ~]# systemctl enable neutron-linuxbridge-agent.service && systemctl restart neutron-linuxbridge-agent.service
Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-linuxbridge-agent.service to /usr/lib/systemd/system/neutron-linuxbridge-agent.service.
[root@linux-node2 ~]# systemctl status openstack-nova-compute.service
[root@linux-node2 ~]# systemctl status neutron-linuxbridge-agent.service
[root@linux-node2 ~]# source /root/admin-openrc.sh
[root@linux-node2 ~]# neutron ext-list
[root@linux-node2 ~]# neutron agent-list
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+
| id                 | agent_type         | host        | availability_zone | alive | admin_state_up | binary              |
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+
| 4d137128-f8b6-48ed | L3 agent           | linux-node1 | nova              | :-)   | True           | neutron-l3-agent    |
| -8172-82a471838f54 |                    |             |                   |       |                |                     |
| 7eff4112-01fa-496e | Linux bridge agent | linux-node2 |                   | :-)   | True           | neutron-            |
| -9249-e2bc301e0474 |                    |             |                   |       |                | linuxbridge-agent   |
| a7e1fe35-0aaf-     | Metadata agent     | linux-node1 |                   | :-)   | True           | neutron-metadata-   |
| 4cae-8e7e-         |                    |             |                   |       |                | agent               |
| f915042548fa       |                    |             |                   |       |                |                     |
| e0974bf1-315e-4eb4 | DHCP agent         | linux-node1 | nova              | :-)   | True           | neutron-dhcp-agent  |
| -a9a9-cda6158e6bac |                    |             |                   |       |                |                     |
| ef2f287e-7414-4626 | Linux bridge agent | linux-node1 |                   | :-)   | True           | neutron-            |
| -a783-5230a93c1d81 |                    |             |                   |       |                | linuxbridge-agent   |
+--------------------+--------------------+-------------+-------------------+-------+----------------+---------------------+



故障
[root@linux-node1 ~]# tail -f /var/log/keystone/keystone.log 
2018-09-04 11:26:55.290 16106 WARNING keystone.common.wsgi [req-796f583b-5aa8-4800-ac81-0a5486f69827 a4441bee92994352a80d08b02ade6b4c - - 723db0cf79e345eb842cfca02ee16403 -] Authorization failed. The request you have made requires authentication. from 192.168.100.151
解决办法:
修改/etc/openstack-dashboard/local-setting文件内容
主要是添加如下内容
OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST

故障:
[root@linux-node1 ~]# tail -f /var/log/nova/nova-conductor.log 
    raise exception.NoValidHost(reason=reason)

NoValidHost: No valid host was found. There are not enough hosts available.

2018-09-05 11:01:40.570 2992 WARNING nova.scheduler.utils [req-f5043897-11aa-4f3d-82a7-c55320193306 f2f2ab1cb6d148eba9538806a7a32e09 bff26680394940569ebf7dd7bdb2e9a1 - - -] [instance: dece1887-a4c2-4894-bd09-5a1ba37c3e96] Setting instance to ERROR state.
2018-09-05 11:01:40.722 2992 WARNING oslo_config.cfg [req-f5043897-11aa-4f3d-82a7-c55320193306 f2f2ab1cb6d148eba9538806a7a32e09 bff26680394940569ebf7dd7bdb2e9a1 - - -] Option "auth_plugin" from group "neutron" is deprecated. Use option "auth_type" from group "neutron".

[root@linux-node1 ~]# tail -f /var/log/nova/nova-compute.log
2018-09-05 11:01:38.714 3114 WARNING nova.virt.osinfo [req-f5043897-11aa-4f3d-82a7-c55320193306 f2f2ab1cb6d148eba9538806a7a32e09 bff26680394940569ebf7dd7bdb2e9a1 - - -] Cannot find OS information - Reason: (No configuration information found for operating system Empty)

解决办法：修改/etc/nova/nova.conf
将
[neutron]
url = http://controller:9696
auth_url = http://controller:35357
auth_plugin = password              《--- 有问题
project_domain_id = default         《--- 有问题
user_domain_id = default            《--- 有问题
region_name = RegionOne
project_name = service
username = neutron
password = neutron
service_metadata_proxy = True
metadata_proxy_shared_secret = 123456
修改为:
[neutron]
url = http://controller:9696
auth_url = http://controller:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = neutron
service_metadata_proxy = True
metadata_proxy_shared_secret = 123456

[root@linux-node1 ~]# systemctl restart openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
[root@linux-node1 ~]# systemctl status openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.servic

